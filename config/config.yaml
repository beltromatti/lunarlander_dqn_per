# Training parameters for the LunarLander DQN project
# Each parameter is explained to illustrate its role in the Deep Q-Network (DQN) algorithm

episodes: 1500
# Number of episodes to train the DQN agent.
# An episode is a single run of the LunarLander environment until the pole falls or a timeout is reached.
# 1000-1500 episodes is typically sufficient to train the agent to achieve high rewards (e.g., >200) in LunarLander-v3.
# Increasing this value allows more training time but increases computational cost.

batch_size: 64
# Number of transitions sampled from the replay memory for each optimization step.
# DQN uses experience replay to learn from past experiences, sampling a batch of transitions (state, action, reward, next_state, done).
# A batch size of 64 balances computational efficiency and stable learning by providing a diverse set of experiences.

gamma: 0.99
# Discount factor for future rewards in the Bellman equation.
# Gamma determines how much the agent values future rewards compared to immediate rewards (0 < gamma <= 1).
# A value of 0.99 means the agent heavily considers future rewards, encouraging long-term planning in LunarLander.

epsilon_start: 1.0
# Initial value of epsilon for the epsilon-greedy exploration strategy.
# Epsilon controls the trade-off between exploration (random actions) and exploitation (actions based on Q-values).
# Starting at 1.0 means the agent initially takes random actions 100% of the time, promoting exploration.

epsilon_end: 0.02
# Final value of epsilon after decay.
# As training progresses, epsilon decreases to favor exploitation over exploration.
# A minimum of 0.02 ensures the agent retains a small amount of randomness (2% random actions) to avoid getting stuck in suboptimal policies.

epsilon_decay_mode: "exponential" 
# Epsilon Decay Mode

epsilon_exponential_decay: 0.995
# Decay rate for epsilon per episode.
# After each episode, epsilon is multiplied by this value (epsilon = max(epsilon_end, epsilon * epsilon_decay)).
# A decay rate of 0.995 reduces epsilon gradually, allowing sufficient exploration early in training while shifting to exploitation later.

memory_size: 20000
# Maximum number of transitions stored in the replay memory.
# The replay memory holds past experiences to break temporal correlations and improve learning stability.
# A size of 20,000 is large enough to store diverse experiences from many episodes in LunarLander, ensuring robust training.

learning_rate: 0.00025
# Learning rate for the Adam optimizer used to update the DQN's weights.
# The learning rate controls the step size of weight updates during backpropagation.
# A small value like 0.00025 ensures stable convergence, preventing large updates that could destabilize learning.

target_update: 5000
# Frequency (in steps) for updating the target network's weights.
# DQN uses a target network to stabilize Q-value estimates in the Bellman equation.
# Copying the main network's weights to the target network every 5000 steps balances stability and adaptation to new learning.

save_checkpoint_every: 50
# Frequency (in episodes) for saving model checkpoints.
# Checkpoints include the model weights, optimizer state, and training progress (episode, epsilon, rewards).
# Saving every 50 episodes allows resuming training from recent states without excessive disk usage.

checkpoint_path: "data/checkpoints/lunarlander_dqn_per.model"
# Path to save model

plot_path: "data/results/lunarlander_dqn_per_rewards.png"
# Path to save rewards plot

use_per: True
# Use Prioritized experience replay

per_alpha: 0.4
# PER Alpha

per_beta_start: 0.6
# PER initial Beta value

per_beta_end: 1
# PER final Beta value

per_beta_annealing_steps: 100000
# Number of steps to bring Beta from beta_start to beta_end